{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be914b35",
   "metadata": {},
   "source": [
    "Scikit-learn (sklearn) provides a powerful feature called \"Pipeline\" that allows you to chain multiple data processing steps together, such as data preprocessing, feature selection, and model training, into a single object. The Pipeline simplifies the process of building and deploying machine learning models by encapsulating all the necessary steps within a single entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a79668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the steps in the pipeline\n",
    "steps = [\n",
    "    ('scaler', StandardScaler()),            # Step 1: Data preprocessing (scaling)\n",
    "    ('feature_selection', SelectKBest()),    # Step 2: Feature selection\n",
    "    ('model', LogisticRegression())          # Step 3: Model training\n",
    "]\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = pipeline.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9759485d",
   "metadata": {},
   "source": [
    "In the above example, we create a pipeline with three steps:\n",
    "\n",
    "* Data preprocessing: We use the StandardScaler to standardize the features by removing the mean and scaling to unit variance.\n",
    "* Feature selection: We use SelectKBest to select the top K features based on some scoring function.\n",
    "* Model training: We use LogisticRegression as our classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c76139e",
   "metadata": {},
   "source": [
    "## Example 1: Text Classification with TF-IDF and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2f0194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the steps in the pipeline\n",
    "steps = [\n",
    "    ('tfidf', TfidfVectorizer()),           # Step 1: Text preprocessing (TF-IDF)\n",
    "    ('model', RandomForestClassifier())     # Step 2: Model training (Random Forest)\n",
    "]\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = pipeline.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ea6d95",
   "metadata": {},
   "source": [
    "## Example 2: Data Preprocessing and Support Vector Machine (SVM) Classification python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55cc3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define the steps in the pipeline\n",
    "steps = [\n",
    "    ('scaler', StandardScaler()),    # Step 1: Data preprocessing (scaling)\n",
    "    ('model', SVC())                 # Step 2: Model training (Support Vector Machine)\n",
    "]\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = pipeline.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae75056a",
   "metadata": {},
   "source": [
    "## Example 3: Feature Union and Gradient Boosting Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bda1d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Define the steps for feature union\n",
    "features = [\n",
    "    ('pca', PCA(n_components=3)),                        # Step 1: Dimensionality reduction (PCA)\n",
    "    ('select_k_best', SelectKBest(k=6)),                  # Step 2: Feature selection\n",
    "]\n",
    "\n",
    "# Define the steps in the pipeline\n",
    "steps = [\n",
    "    ('feature_union', FeatureUnion(features)),            # Step 3: Combine features\n",
    "    ('scaler', StandardScaler()),                         # Step 4: Data preprocessing (scaling)\n",
    "    ('model', GradientBoostingRegressor())                # Step 5: Model training (Gradient Boosting)\n",
    "]\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386779ad",
   "metadata": {},
   "source": [
    "## Example 4: Imputation and Classification with Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd518069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Define the steps in the pipeline\n",
    "steps = [\n",
    "    ('imputer', SimpleImputer()),                 # Step 1: Data imputation\n",
    "    ('model', DecisionTreeClassifier())           # Step 2: Model training (Decision Tree)\n",
    "]\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = pipeline.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19206b99",
   "metadata": {},
   "source": [
    "## Example 5: Scaling and Clustering with K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca8d1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Define the steps in the pipeline\n",
    "steps = [\n",
    "    ('scaler', StandardScaler()),                # Step 1: Data preprocessing (scaling)\n",
    "    ('model', KMeans(n_clusters=3))              # Step 2: Model training (K-Means Clustering)\n",
    "]\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Perform cluster assignment on new data\n",
    "new_data = scaler.transform(new_data)\n",
    "new_predictions = pipeline.predict(new_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66aa28f1",
   "metadata": {},
   "source": [
    "## Example 6: Feature Extraction and Dimensionality Reduction with PCA and Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f633a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Define the steps in the pipeline\n",
    "steps = [\n",
    "    ('pca', PCA(n_components=10)),               # Step 1: Dimensionality reduction (PCA)\n",
    "    ('model', LinearRegression())                # Step 2: Model training (Linear Regression)\n",
    "]\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc35ad6",
   "metadata": {},
   "source": [
    "## Example 7: Preprocessing, Feature Selection, and Ensemble Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956ad269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the steps in the pipeline\n",
    "steps = [\n",
    "    ('scaler', StandardScaler()),                  # Step 1: Data preprocessing (scaling)\n",
    "    ('feature_selection', SelectKBest(k=10)),      # Step 2: Feature selection\n",
    "    ('model', RandomForestClassifier())            # Step 3: Model training (Random Forest)\n",
    "]\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = pipeline.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b7fde7",
   "metadata": {},
   "source": [
    "## Example 8: Preprocessing, Feature Transformation, and Regression with Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fadc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Define the steps in the pipeline\n",
    "steps = [\n",
    "    ('scaler', StandardScaler()),                        # Step 1: Data preprocessing (scaling)\n",
    "    ('polynomial_features', PolynomialFeatures(degree=2)),  # Step 2: Feature transformation\n",
    "    ('model', Ridge(alpha=0.5))                            # Step 3: Model training (Ridge Regression)\n",
    "]\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef509e8",
   "metadata": {},
   "source": [
    "## Example 9: Preprocessing, Feature Encoding, and Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a460ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Define the steps in the pipeline\n",
    "steps = [\n",
    "    ('scaler', StandardScaler()),                        # Step 1: Data preprocessing (scaling)\n",
    "    ('encoding', OneHotEncoder()),                        # Step 2: Feature encoding\n",
    "    ('model', GradientBoostingClassifier())                # Step 3: Model training (Gradient Boosting)\n",
    "]\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = pipeline.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69b064b",
   "metadata": {},
   "source": [
    "## Example 10: Feature Scaling, Feature Selection, and Support Vector Machine (SVM) Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44dcf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Define the steps in the pipeline\n",
    "steps = [\n",
    "    ('scaler', StandardScaler()),                 # Step 1: Data preprocessing (scaling)\n",
    "    ('feature_selection', SelectKBest(k=5)),      # Step 2: Feature selection\n",
    "    ('model', SVR())                              # Step 3: Model training (Support Vector Regression)\n",
    "]\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46152ace",
   "metadata": {},
   "source": [
    "## Example 11: Text Preprocessing, Feature Extraction, and Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a799e2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Define the steps in the pipeline\n",
    "steps = [\n",
    "    ('vectorizer', TfidfVectorizer()),               # Step 1: Text preprocessing and feature extraction\n",
    "    ('model', MultinomialNB())                        # Step 2: Model training (Multinomial Naive Bayes)\n",
    "]\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = pipeline.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b796336",
   "metadata": {},
   "source": [
    "## Example 12: Data Imputation, Feature Scaling, and Ensemble Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13be89fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Define the steps in the pipeline\n",
    "steps = [\n",
    "    ('imputer', SimpleImputer()),                    # Step 1: Data imputation\n",
    "    ('scaler', StandardScaler()),                     # Step 2: Data preprocessing (scaling)\n",
    "    ('model', RandomForestRegressor())               # Step 3: Model training (Random Forest Regression)\n",
    "]\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767a82a8",
   "metadata": {},
   "source": [
    "## Example 13: Preprocessing, Feature Transformation, and Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec9d9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Define the steps in the pipeline\n",
    "steps = [\n",
    "    ('scaler', StandardScaler()),                       # Step 1: Data preprocessing (scaling)\n",
    "    ('pca', PCA(n_components=10)),                      # Step 2: Feature transformation (PCA)\n",
    "    ('model', GradientBoostingClassifier())             # Step 3: Model training (Gradient Boosting Classifier)\n",
    "]\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = pipeline.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c533ee",
   "metadata": {},
   "source": [
    "## Example 14: Preprocessing, Feature Encoding, and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074e1766",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the steps in the pipeline\n",
    "steps = [\n",
    "    ('scaler', StandardScaler()),                       # Step 1: Data preprocessing (scaling)\n",
    "    ('encoder', OneHotEncoder()),                        # Step 2: Feature encoding\n",
    "    ('model', LogisticRegression())                     # Step 3: Model training (Logistic Regression)\n",
    "]\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = pipeline.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e95124",
   "metadata": {},
   "source": [
    "## Example 15: Preprocessing, Dimensionality Reduction, and K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2552e65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Define the steps in the pipeline\n",
    "steps = [\n",
    "    ('scaler', StandardScaler()),                       # Step 1: Data preprocessing (scaling)\n",
    "    ('pca', PCA(n_components=2)),                        # Step 2: Dimensionality reduction (PCA)\n",
    "    ('model', KMeans(n_clusters=3))                      # Step 3: Model training (K-Means Clustering)\n",
    "]\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Perform cluster assignment on new data\n",
    "new_data = scaler.transform(new_data)\n",
    "new_predictions = pipeline.predict(new_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6913f3",
   "metadata": {},
   "source": [
    "## Example 16: Preprocessing, Feature Extraction, and Support Vector Machine (SVM) Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24b34c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define the steps in the pipeline\n",
    "steps = [\n",
    "    ('scaler', StandardScaler()),                       # Step 1: Data preprocessing (scaling)\n",
    "    ('vectorizer', TfidfVectorizer()),                  # Step 2: Feature extraction\n",
    "    ('model', SVC())                                    # Step 3: Model training (SVM Classification)\n",
    "]\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = pipeline.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ced82a",
   "metadata": {},
   "source": [
    "## Example 17: Preprocessing, Dimensionality Reduction, and Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569d653d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Define the steps in the pipeline\n",
    "steps = [\n",
    "    ('scaler', StandardScaler()),                       # Step 1: Data preprocessing (scaling)\n",
    "    ('pca', PCA(n_components=10)),                      # Step 2: Dimensionality reduction (PCA)\n",
    "    ('model', RandomForestRegressor())                  # Step 3: Model training (Random Forest Regression)\n",
    "]\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2137ca8d",
   "metadata": {},
   "source": [
    "## Example 18: Preprocessing, Feature Encoding, and Multiclass Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a967fa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the steps in the pipeline\n",
    "steps = [\n",
    "    ('scaler', StandardScaler()),                       # Step 1: Data preprocessing (scaling)\n",
    "    ('encoder', LabelEncoder()),                         # Step 2: Feature encoding\n",
    "    ('model', LogisticRegression(multi_class='multinomial'))  # Step 3: Model training (Multiclass Logistic Regression)\n",
    "]\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = pipeline.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10736bbc",
   "metadata": {},
   "source": [
    "# Some Advance Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc1e145",
   "metadata": {},
   "source": [
    "## Example 1: Preprocessing, Feature Selection, Model Selection, and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bfffeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define the steps in the pipeline\n",
    "steps = [\n",
    "    ('scaler', StandardScaler()),                          # Step 1: Data preprocessing (scaling)\n",
    "    ('feature_selection', SelectKBest()),                  # Step 2: Feature selection\n",
    "    ('model', SVC())                                       # Step 3: Model training (SVM Classification)\n",
    "]\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'feature_selection__k': [5, 10, 15],                   # Parameter options for feature selection\n",
    "    'model__C': [0.1, 1, 10],                              # Parameter options for the SVM model\n",
    "}\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model and its parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Make predictions with the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = best_model.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdb4e4f",
   "metadata": {},
   "source": [
    "## Example 2: Text Preprocessing, Feature Extraction, Model Selection, and Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913cb28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the steps in the pipeline\n",
    "steps = [\n",
    "    ('vectorizer', TfidfVectorizer()),                     # Step 1: Text preprocessing and feature extraction\n",
    "    ('model', VotingClassifier(estimators=[                 # Step 2: Model selection and ensembling\n",
    "        ('svm', SVC()),\n",
    "        ('nb', MultinomialNB()),\n",
    "        ('lr', LogisticRegression())\n",
    "    ]))\n",
    "]\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'vectorizer__ngram_range': [(1, 1), (1, 2)],            # Parameter options for n-gram range\n",
    "    'model__svm__C': [0.1, 1, 10],                          # Parameter options for the SVM model\n",
    "    'model__nb__alpha': [0.1, 0.5, 1],                      # Parameter options for the Naive Bayes model\n",
    "    'model__lr__C': [0.1, 1, 10]                            # Parameter options for the Logistic Regression model\n",
    "}\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model and its parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Make predictions with the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = best_model.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bda1765",
   "metadata": {},
   "source": [
    "## Example 3: Preprocessing, Feature Engineering, and Neural Network Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c437a2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Define the steps in the pipeline\n",
    "steps = [\n",
    "    ('scaler', StandardScaler()),                      # Step 1: Data preprocessing (scaling)\n",
    "    ('pca', PCA(n_components=10)),                     # Step 2: Dimensionality reduction (PCA)\n",
    "    ('model', MLPClassifier(hidden_layer_sizes=(50,)))  # Step 3: Model training (Neural Network Classification)\n",
    "]\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = pipeline.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298b91f8",
   "metadata": {},
   "source": [
    "## Example 4: Preprocessing, Feature Encoding, and Gradient Boosting Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6a303a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Define the steps in the pipeline\n",
    "steps = [\n",
    "    ('scaler', StandardScaler()),                      # Step 1: Data preprocessing (scaling)\n",
    "    ('encoder', OneHotEncoder()),                       # Step 2: Feature encoding\n",
    "    ('model', GradientBoostingRegressor())             # Step 3: Model training (Gradient Boosting Regression)\n",
    "]\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8d03b9",
   "metadata": {},
   "source": [
    "## Example 5: Preprocessing, Feature Extraction, and Gaussian Mixture Model (GMM) Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40654ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Define the steps in the pipeline\n",
    "steps = [\n",
    "    ('scaler', StandardScaler()),                      # Step 1: Data preprocessing (scaling)\n",
    "    ('pca', PCA(n_components=2)),                       # Step 2: Dimensionality reduction (PCA)\n",
    "    ('model', GaussianMixture(n_components=3))        # Step 3: Model training (GMM Clustering)\n",
    "]\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Perform cluster assignment on new data\n",
    "new_data = scaler.transform(new_data)\n",
    "new_predictions = pipeline.predict(new_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842e794e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aeb749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12e1b4e2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68044c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
